library("gbm")
library("lattice")
set.seed(3)	# se establece la semilla
datos <- read.csv("./data/parkinsons_updrs.data", head=T)
datos
train <- as.data.frame(datos[datos$subject. <  34,])
test  <- as.data.frame(datos[datos$subject. >= 34,])
train
test
train[,dim(train)[2]] <- as.factor(train[,1])
test[,dim(test)[2]] <- as.factor(test[,1])
cat("Dim train (personas 1-30): ", dim(train), "\n")
cat("Dim test  (personas 30-42): ", dim(test ), "\n")
nzv1 <- nearZeroVar(train)
nzv2 <- nearZeroVar(test)
train <- train[,-nzv1]
test  <- test [,-nzv2]
cat("Dim train (personas 1-30): ", dim(train), "\n")
cat("Dim test  (personas 30-42): ", dim(test ), "\n")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library("tinytex")
library("rmarkdown")
install.packages("tinytex")
install.packages("rmarkdown")
install.packages("caret","rpart")
knitr::opts_chunk$set(echo = TRUE)
library("tinytex")
library("rmarkdown")
library("caret")
library("caret"
)
library("caret")
install.packages("caret")
knitr::opts_chunk$set(echo = TRUE)
library("tinytex")
library("rmarkdown")
library("caret")
library("rpart")
install.packages("leaps")
install.packa ges("e1071")
knitr::opts_chunk$set(echo = TRUE)
library("tinytex")
library("rmarkdown")
library("caret")
library("rpart")
install.packages("leaps")
install.packages("e1071")
install.packages("maptree")
install.packages("FNN")
install.packages("party")
install.packages("gbm")
library("lattice")
install.packages("randomForest")
library("MASS")
set.seed(3)	# se establece la semilla
knitr::opts_chunk$set(echo = TRUE)
library("tinytex")
library("rmarkdown")
library("caret")
library("rpart")
library("leaps")
library("e1071")
library("maptree")
library("FNN")
library("party")
library("gbm")
library("lattice")
library("randomForest")
library("MASS")
set.seed(3)	# se establece la semilla
# Aprendemos del train
modelo.svm1 = svm(motor_UPDRS ~ . - total_UPDRS, data = train)
perdidos.train <- sum(is.na(train))
perdidos.test  <- sum(is.na(test ))
# Eliminamos: sujeto, edad, sexo y test_time:
datos <- datos[,-(1:4)]
datos <- read.csv("./data/parkinsons_updrs.data", head=T, sep = ",")
# Eliminamos: sujeto, edad, sexo y test_time:
datos <- datos[,-(1:4)]
summary(datos)
# Elegimos los indices para el train, 80% de los datos
idx.Train = sample(nrow(datos),size = nrow(datos)*0.8)
# Formamos loc conjuntos train y test.
train <- as.data.frame(datos[idx.Train,])
test  <- as.data.frame(datos[-idx.Train,])
cat("Dim train : ", dim(train), "\n")
cat("Dim test  : ", dim(test ), "\n")
nearZeroVar(train,saveMetrics = TRUE)
nearZeroVar(test,saveMetrics = TRUE)
nearZeroVar(train,saveMetrics = TRUE)
nearZeroVar(test,saveMetrics = TRUE)
perdidos.train <- sum(is.na(train))
perdidos.test  <- sum(is.na(test ))
cat("Num valores perdidos: ", perdidos.train, ",", perdidos.test,"\n")
perdidos.train <- sum(is.na(train))
perdidos.test  <- sum(is.na(test ))
print("Num valores perdidos: ", perdidos.train, ",", perdidos.test,"\n")
perdidos.train <- sum(is.na(train))
perdidos.test  <- sum(is.na(test ))
print("Num valores perdidos: " perdidos.train "," perdidos.test,"\n")
perdidos.train <- sum(is.na(train))
perdidos.test  <- sum(is.na(test ))
cat("Num valores perdidos: ", perdidos.train, ",", perdidos.test,"\n")
# Aprendemos del train
modelo.lm1 = lm(motor_UPDRS ~ . - total_UPDRS, data = train)
modelo.lm2 = lm(total_UPDRS ~ . - motor_UPDRS, data = train)
# Mostramos la interpretación que ha hecho el modelo
par (mfrow = c (2,2))
plot(modelo.lm1)
plot(modelo.lm2)
# Aprendemos del train
modelo.svm1 = svm(motor_UPDRS ~ . - total_UPDRS, data = train)
modelo.svm2 = svm(total_UPDRS ~ . - motor_UPDRS, data = train)
# Calculamos gamma
svm_tune <- tune(modelo.svm1 ~ ., data = train,
ranges = list(epsilon = seq(0,1,0.01), cost = 2^(2:9))
)
# Aprendemos del train
modelo.svm1 = svm(motor_UPDRS ~ . - total_UPDRS, data = train)
modelo.svm2 = svm(total_UPDRS ~ . - motor_UPDRS, data = train)
# Calculamos gamma
svm_tune <- tune(modelo.svm1, data = train,
ranges = list(epsilon = seq(0,1,0.01), cost = 2^(2:9))
)
# Aprendemos del train
modelo.svm1 = svm(motor_UPDRS ~ . - total_UPDRS, data = train)
modelo.svm2 = svm(total_UPDRS ~ . - motor_UPDRS, data = train)
print(modelo.svm1)
# Calculamos gamma
svm_tune <- tune(modelo.svm1, data = train,
ranges = list(epsilon = seq(0,1,0.01), cost = 2^(2:9))
)
# Aprendemos del train
modelo.svm1 = svm(motor_UPDRS ~ . - total_UPDRS, data = train)
modelo.svm2 = svm(total_UPDRS ~ . - motor_UPDRS, data = train)
print(modelo.svm1)
print(modelo.svm2)
# Calculamos gamma
svm_tune <- tune(modelo.svm1, data = train,
ranges = list(epsilon = seq(0,1,0.01), cost = 2^(2:9))
)
# Aprendemos del train
modelo.svm1 = svm(motor_UPDRS ~ . - total_UPDRS, data = train)
modelo.svm2 = svm(total_UPDRS ~ . - motor_UPDRS, data = train)
print(modelo.svm1)
print(modelo.svm2)
# Calculamos gamma
svm_tune <- tune(modelo.svm1, motor_UPDRS ~ . - total_UPDRS, data = train,
ranges = list(epsilon = seq(0,1,0.01), cost = 2^(2:9))
)
# Aprendemos del train
obj1 = best.tune(svm, motor_UPDRS ~ . - total_UPDRS, data = train)
obj2 = best.tune(svm, total_UPDRS ~ . - motor_UPDRS, data = train)
modelo.svm1 = svm(motor_UPDRS ~ . - total_UPDRS, data = train)
modelo.svm2 = svm(total_UPDRS ~ . - motor_UPDRS, data = train)
print(modelo.svm1)
print(modelo.svm2)
# Calculamos gamma
svm_tune <- tune(motor_UPDRS ~ . - total_UPDRS, data = train,
ranges = list(epsilon = seq(0,1,0.01), cost = 2^(2:9))
)
print(obj1)
print(obj2)
modelo.svm1 = svm(motor_UPDRS ~ . - total_UPDRS, data = train)
modelo.svm2 = svm(total_UPDRS ~ . - motor_UPDRS, data = train)
print(modelo.svm1)
print(modelo.svm2)
# Calculamos gamma
svm_tune <- tune(motor_UPDRS ~ . - total_UPDRS, data = train,
ranges = list(epsilon = seq(0,1,0.01), cost = 2^(2:9))
)
# Aprendemos del train
obj1 = best.tune(svm, motor_UPDRS ~ . - total_UPDRS, data = train,
ranges = list(epsilon = seq(0,1,0.01), cost = 2^(2:9)))
knitr::opts_chunk$set(echo = TRUE)
library("tinytex")
library("rmarkdown")
library("caret")
library("rpart")
library("leaps")
library("e1071")
library("maptree")
library("FNN")
library("party")
library("gbm")
library("lattice")
library("randomForest")
library("MASS")
set.seed(3)	# se establece la semilla
datos <- read.csv("./data/parkinsons_updrs.data", head=T, sep = ",")
# Eliminamos: sujeto, edad, sexo y test_time:
datos <- datos[,-(1:4)]
summary(datos)
# Elegimos los indices para el train, 80% de los datos
idx.Train = sample(nrow(datos),size = nrow(datos)*0.8)
# Formamos loc conjuntos train y test.
train <- as.data.frame(datos[idx.Train,])
test  <- as.data.frame(datos[-idx.Train,])
cat("Dim train : ", dim(train), "\n")
cat("Dim test  : ", dim(test ), "\n")
nearZeroVar(train,saveMetrics = TRUE)
nearZeroVar(test,saveMetrics = TRUE)
perdidos.train <- sum(is.na(train))
perdidos.test  <- sum(is.na(test ))
cat("Num valores perdidos: ", perdidos.train, ",", perdidos.test,"\n")
# Aprendemos del train
modelo.lm1 = lm(motor_UPDRS ~ . - total_UPDRS, data = train)
modelo.lm2 = lm(total_UPDRS ~ . - motor_UPDRS, data = train)
# Mostramos la interpretación que ha hecho el modelo
par (mfrow = c (2,2))
plot(modelo.lm1)
plot(modelo.lm2)
# Aprendemos del train
obj1 = best.tune(svm, motor_UPDRS ~ . - total_UPDRS, data = train,
ranges = list(epsilon = seq(0,1,0.01), cost = 2^(2:9)))
# Aprendemos cual es el mejor valor libre de dos cifras.
obj1 = best.tune(svm, motor_UPDRS ~ . - total_UPDRS, data = train,
ranges = list(epsilon = seq(0,1,0.01), cost = 2^(2:9)))
# Aprendemos cual es el mejor valor libre de dos cifras.
obj1 = best.tune(svm, motor_UPDRS ~ . - total_UPDRS, data = train,
ranges = list(epsilon = seq(0,1,0.01), cost = 2^(2:9)))
# Aprendemos cual es el mejor valor libre de dos cifras.
obj1 = best.tune(svm, motor_UPDRS ~ . - total_UPDRS, data = train,
ranges = list(epsilon = seq(0,1,0.01), cost = 1))
# Aprendemos cual es el mejor valor libre de dos cifras.
obj1 = best.tune(svm, motor_UPDRS ~ . - total_UPDRS, data = train,
ranges = list(epsilon = seq(0,1,0.01)))
# Aprendemos cual es el mejor valor libre de dos cifras.
obj1 = best.tune(svm, motor_UPDRS ~ . - total_UPDRS, data = train)
obj2 = best.tune(svm, total_UPDRS ~ . - motor_UPDRS, data = train)
print(obj1)
print(obj2)
# Aprendemos del train usando ese gamma
modelo.svm1 = svm(motor_UPDRS ~ . - total_UPDRS,
data = train, gamma = 0.06, epsilon = 0.1)
modelo.svm2 = svm(total_UPDRS ~ . - motor_UPDRS,
data = train, gamma = 0.06, epsilon = 0.1)
print(modelo.svm1)
print(modelo.svm2)
modelo.gbm1 = gbm(motor_UPDRS ~ . - total_UPDRS, data = train)
modelo.gbm2 = gbm(total_UPDRS ~ . - motor_UPDRS, data = train)
plot(modelo.gbm1)
plot(modelo.gbm2)
# Aprendemos el modelo
modelo.rf1 = randomForest(motor_UPDRS ~ . - total_UPDRS, data = train)
modelo.rf2 = randomForest(total_UPDRS ~ . - motor_UPDRS, data = train)
# Pintamos el modelo
par (mfrow = c (1,2))
plot(modelo.rf1)
plot(modelo.rf2)
modelo.gbm1 = gbm(motor_UPDRS ~ . - total_UPDRS, data = train)
modelo.gbm2 = gbm(total_UPDRS ~ . - motor_UPDRS, data = train)
print(modelo.gbm1)
print(modelo.gbm2)
plot(modelo.gbm1)
plot(modelo.gbm2)
# Pintamos el modelo
print(modelo.rf1)
print(modelo.rf2)
par (mfrow = c (1,2))
plot(modelo.rf1)
plot(modelo.rf2)
# Pintamos el modelo
print(modelo.rf1)
print(modelo.rf2)
par (mfrow = c (1,2))
min(modelo.rf1)
eval_model <- function(model,type=1, Boosting = F) {
ein <- 0
eout <- 0
# Para boosting neceesitamos el número de arboles que van a ser
# usados para pronosticar, que los ajustaremos a 100.
if(Boosting == FALSE){
pred_train <- predict(model,newdata = train)
pred_test <- predict(model,newdata = test)
# Tipo indica si estamos hablando de motor_UDPRS o total_UDPRS
if (type == 1) {
ein <- sqrt(mean((pred_train-train$motor_UPDRS)^2))
eout <- sqrt(mean((pred_test-test$motor_UPDRS)^2))
}else{
ein<- sqrt(mean((pred_train-train$total_UPDRS)^2))
eout<- sqrt(mean((pred_test-test$total_UPDRS)^2))
}
}else{
pred_train <- predict(model,newdata = train, n.trees = 100)
pred_test <- predict(model,newdata = test, n.trees = 100)
if (type == 1) {
ein<- sqrt(mean((pred_train-train$motor_UPDRS)^2))
eout<- sqrt(mean((pred_test-test$motor_UPDRS)^2))
}else{
ein<- sqrt(mean((pred_train-train$total_UPDRS)^2))
eout<- sqrt(mean((pred_test-test$total_UPDRS)^2))
}
}
return (c(ein,eout))
}
nearZeroVar(train)
nearZeroVar(test)
knitr::opts_chunk$set(echo = TRUE)
library("tinytex")
library("rmarkdown")
library("caret")
library("rpart")
library("leaps")
library("e1071")
library("maptree")
library("FNN")
library("party")
library("gbm")
library("lattice")
library("randomForest")
library("MASS")
set.seed(3)	# se establece la semilla
datos <- read.csv("./datos/parkinsons_updrs.data", head=T, sep = ",")
# Eliminamos: sujeto, edad, sexo y test_time:
datos <- datos[,-(1:4)]
summary(datos)
# Elegimos los indices para el train, 80% de los datos
idx.Train = sample(nrow(datos),size = nrow(datos)*0.8)
# Formamos loc conjuntos train y test.
train <- as.data.frame(datos[idx.Train,])
test  <- as.data.frame(datos[-idx.Train,])
cat("Dim train : ", dim(train), "\n")
cat("Dim test  : ", dim(test ), "\n")
Sys.sleep(3)
nearZeroVar(train)
nearZeroVar(test)
perdidos.train <- sum(is.na(train))
perdidos.test  <- sum(is.na(test ))
cat("Num valores perdidos: ", perdidos.train, ",", perdidos.test,"\n")
Sys.sleep(3)
# Aprendemos del train
modelo.lm1 = lm(motor_UPDRS ~ . - total_UPDRS, data = train)
modelo.lm2 = lm(total_UPDRS ~ . - motor_UPDRS, data = train)
# Mostramos la interpretación que ha hecho el modelo
par (mfrow = c (2,2))
plot(modelo.lm1)
plot(modelo.lm2)
Sys.sleep(3)
# Aprendemos cual es el mejor valor libre de dos cifras.
obj1 = best.tune(svm, motor_UPDRS ~ . - total_UPDRS, data = train)
obj2 = best.tune(svm, total_UPDRS ~ . - motor_UPDRS, data = train)
print(obj1)
print(obj2)
# Aprendemos del train usando ese gamma
modelo.svm1 = svm(motor_UPDRS ~ . - total_UPDRS,
data = train, gamma = 0.06, epsilon = 0.1)
modelo.svm2 = svm(total_UPDRS ~ . - motor_UPDRS,
data = train, gamma = 0.06, epsilon = 0.1)
modelo.gbm1 = gbm(motor_UPDRS ~ . - total_UPDRS, data = train)
modelo.gbm2 = gbm(total_UPDRS ~ . - motor_UPDRS, data = train)
print(modelo.gbm1)
print(modelo.gbm2)
plot(modelo.gbm1)
plot(modelo.gbm2)
Sys.sleep(3)
# Aprendemos el modelo
modelo.rf1 = randomForest(motor_UPDRS ~ . - total_UPDRS, data = train)
modelo.rf2 = randomForest(total_UPDRS ~ . - motor_UPDRS, data = train)
par (mfrow = c (1,2))
plot(modelo.rf1)
plot(modelo.rf2)
Sys.sleep(3)
eval_model <- function(model,type=1, Boosting = F) {
ein <- 0
eout <- 0
# Para boosting neceesitamos el número de arboles que van a ser
# usados para pronosticar, que los ajustaremos a 100.
if(Boosting == FALSE){
pred_train <- predict(model,newdata = train)
pred_test <- predict(model,newdata = test)
# Tipo indica si estamos hablando de motor_UDPRS o total_UDPRS
if (type == 1) {
ein <- sqrt(mean((pred_train-train$motor_UPDRS)^2))
eout <- sqrt(mean((pred_test-test$motor_UPDRS)^2))
}else{
ein<- sqrt(mean((pred_train-train$total_UPDRS)^2))
eout<- sqrt(mean((pred_test-test$total_UPDRS)^2))
}
}else{
pred_train <- predict(model,newdata = train, n.trees = 100)
pred_test <- predict(model,newdata = test, n.trees = 100)
if (type == 1) {
ein<- sqrt(mean((pred_train-train$motor_UPDRS)^2))
eout<- sqrt(mean((pred_test-test$motor_UPDRS)^2))
}else{
ein<- sqrt(mean((pred_train-train$total_UPDRS)^2))
eout<- sqrt(mean((pred_test-test$total_UPDRS)^2))
}
}
return (c(ein,eout))
}
eval.lm1 = eval_model(modelo.lm1,1)
eval.lm2 = eval_model(modelo.lm2,2)
eval.svm1 = eval_model(modelo.svm1,1)
eval.svm2 = eval_model(modelo.svm2,2)
eval.rf1 = eval_model(modelo.rf1,1)
eval.rf2 = eval_model(modelo.rf2,2)
eval.gbm1 = eval_model(modelo.gbm1,1,T)
eval.gbm2 = eval_model(modelo.gbm2,2,T)
cat("LM    \n     motor_UPDRS - Ein: ", eval.lm1[1],
"      \n     total_UPDRS - Ein: ", eval.lm2[1],
"\nSVM \n     motor_UPDRS - Ein: ", eval.svm1[1],
"      \n     total_UPDRS - Ein: ", eval.svm2[1],
"\nBST \n     motor_UPDRS - Ein: ", eval.gbm1[1],
"      \n     total_UPDRS - Ein: ", eval.gbm2[1],
"\nRF  \n     motor_UPDRS - Ein: ", eval.rf1[1],
"      \n     total_UPDRS - Ein: ", eval.rf2[1])
Sys.sleep(3)
cat("LM    \n     motor_UPDRS - Eout: ", eval.lm1[2],
"      \n     total_UPDRS - Eout: ", eval.lm2[2],
"\nSVM \n     motor_UPDRS - Eout: ", eval.svm1[2],
"      \n     total_UPDRS - Eout: ", eval.svm2[2],
"\nBST \n     motor_UPDRS - Eout: ", eval.gbm1[2],
"      \n     total_UPDRS - Eout: ", eval.gbm2[2],
"\nRF  \n     motor_UPDRS - Eout: ", eval.rf1[2],
"      \n     total_UPDRS - Eout: ", eval.rf2[2])
Sys.sleep(3)
# Calculamos la transformación
trans = preProcess(train, method = c("center","scale"))
trans2 = preProcess(train, method = c("center","scale", "pca"))
# Obtenemos el nuevo conunto de datos.
train = predict(trans, train)
test = predict(trans, test)
train.pca = predict(trans2, train)
test.pca = predict(trans2, test)
cat("Dim Train con PCA: ", dim(train.pca))
Sys.sleep(3)
# Aprendemos de nuevo los modelos
modelo2.lm1 = lm(motor_UPDRS ~ . - total_UPDRS, data = train)
modelo2.lm2 = lm(total_UPDRS ~ . - motor_UPDRS, data = train)
modelo2.svm1 = svm(motor_UPDRS ~ . - total_UPDRS, data = train)
modelo2.svm2 = svm(total_UPDRS ~ . - motor_UPDRS, data = train)
modelo2.rf1 = randomForest(motor_UPDRS ~ . - total_UPDRS, data = train)
modelo2.rf2 = randomForest(total_UPDRS ~ . - motor_UPDRS, data = train)
modelo2.gbm1 = gbm(motor_UPDRS ~ . - total_UPDRS, data = train)
modelo2.gbm2 = gbm(total_UPDRS ~ . - motor_UPDRS, data = train)
# Evaluamos los modelos
eval2.lm1 = eval_model(modelo2.lm1,1)
eval2.lm2 = eval_model(modelo2.lm2,2)
eval2.svm1 = eval_model(modelo2.svm1,1)
eval2.svm2 = eval_model(modelo2.svm2,2)
eval2.rf1 = eval_model(modelo2.rf1,1)
eval2.rf2 = eval_model(modelo2.rf2,2)
eval2.gbm1 = eval_model(modelo2.gbm1,1,T)
eval2.gbm2 = eval_model(modelo2.gbm2,2,T)
# Mostramos el Ein
# -----------------------------------------------------
cat("LM    \n     motor_UPDRS - Ein: ", eval2.lm1[1],
"      \n     total_UPDRS - Ein: ", eval2.lm2[1],
"\nSVM \n     motor_UPDRS - Ein: ", eval2.svm1[1],
"      \n     total_UPDRS - Ein: ", eval2.svm2[1],
"\nBST \n     motor_UPDRS - Ein: ", eval2.gbm1[1],
"      \n     total_UPDRS - Ein: ", eval2.gbm2[1],
"\nRF  \n     motor_UPDRS - Ein: ", eval2.rf1[1],
"      \n     total_UPDRS - Ein: ", eval2.rf2[1])
Sys.sleep(3)
# Mostramos el Eout
# ------------------------------------------------------
cat("LM    \n     motor_UPDRS - Eout: ", eval2.lm1[2],
"      \n     total_UPDRS - Eout: ", eval2.lm2[2],
"\nSVM \n     motor_UPDRS - Eout: ", eval2.svm1[2],
"      \n     total_UPDRS - Eout: ", eval2.svm2[2],
"\nBST \n     motor_UPDRS - Eout: ", eval2.gbm1[2],
"      \n     total_UPDRS - Eout: ", eval2.gbm2[2],
"\nRF  \n     motor_UPDRS - Eout: ", eval2.rf1[2],
"      \n     total_UPDRS - Eout: ", eval2.rf2[2])
Sys.sleep(3)
# Aprendemos de nuevo los modelos
modelo2.lm1 = lm(motor_UPDRS ~ . - total_UPDRS, data = train)
modelo2.lm2 = lm(total_UPDRS ~ . - motor_UPDRS, data = train)
modelo2.svm1 = svm(motor_UPDRS ~ . - total_UPDRS,
data = train, gamma = 0.06, epsilon = 0.1)
modelo2.svm2 = svm(total_UPDRS ~ . - motor_UPDRS,
data = train, gamma = 0.06, epsilon = 0.1)
modelo2.rf1 = randomForest(motor_UPDRS ~ . - total_UPDRS, data = train)
modelo2.rf2 = randomForest(total_UPDRS ~ . - motor_UPDRS, data = train)
modelo2.gbm1 = gbm(motor_UPDRS ~ . - total_UPDRS, data = train)
modelo2.gbm2 = gbm(total_UPDRS ~ . - motor_UPDRS, data = train)
# Evaluamos los modelos
eval2.lm1 = eval_model(modelo2.lm1,1)
eval2.lm2 = eval_model(modelo2.lm2,2)
eval2.svm1 = eval_model(modelo2.svm1,1)
eval2.svm2 = eval_model(modelo2.svm2,2)
eval2.rf1 = eval_model(modelo2.rf1,1)
eval2.rf2 = eval_model(modelo2.rf2,2)
eval2.gbm1 = eval_model(modelo2.gbm1,1,T)
eval2.gbm2 = eval_model(modelo2.gbm2,2,T)
# Mostramos el Ein
# -----------------------------------------------------
cat("LM    \n     motor_UPDRS - Ein: ", eval2.lm1[1],
"      \n     total_UPDRS - Ein: ", eval2.lm2[1],
"\nSVM \n     motor_UPDRS - Ein: ", eval2.svm1[1],
"      \n     total_UPDRS - Ein: ", eval2.svm2[1],
"\nBST \n     motor_UPDRS - Ein: ", eval2.gbm1[1],
"      \n     total_UPDRS - Ein: ", eval2.gbm2[1],
"\nRF  \n     motor_UPDRS - Ein: ", eval2.rf1[1],
"      \n     total_UPDRS - Ein: ", eval2.rf2[1])
Sys.sleep(3)
# Mostramos el Eout
# ------------------------------------------------------
cat(
"\nLM  \n     motor_UPDRS - Eout: ", eval2.lm1[2],
"      \n     total_UPDRS - Eout: ", eval2.lm2[2],
"\nSVM \n     motor_UPDRS - Eout: ", eval2.svm1[2],
"      \n     total_UPDRS - Eout: ", eval2.svm2[2],
"\nBST \n     motor_UPDRS - Eout: ", eval2.gbm1[2],
"      \n     total_UPDRS - Eout: ", eval2.gbm2[2],
"\nRF  \n     motor_UPDRS - Eout: ", eval2.rf1[2],
"      \n     total_UPDRS - Eout: ", eval2.rf2[2])
Sys.sleep(3)
